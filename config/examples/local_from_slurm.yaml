defaults:
  - override hydra/job_logging: disabled

_target_: src.types.ExperimentConfig
num_episodes: 1
env_seed: [0]
verbose: false
baseline_data_dir: ../embedding_models/
exp_name: local_from_slurm_example
match_encoder_decoder: false
fixed_interceptor: "GloVe"
filter_model: ""
get_models_from_slurm: true # Will get local models from slurm queue (`squeue --me`)
max_tokens: 1000 # Sets max_tokens for models taken from slurm queue
temperature: 0.6 # Sets temperature for models taken from slurm queue
models:
  - _target_: src.types.BaselineModel  # BaselineModels are not in the slurm queue
    model_key: "GloVe"
    model_id: "GloVe"
    global_guess: true
    baseline_k: 16
    model_seed: 0

  # - _target_: src.types.LocalModel   # <--- models specified here and in the slurm queue will be included twice
  #   model_key: llama3.1_8B
  #   model_id: meta-llama/Meta-Llama-3.1-8B-Instruct
  #   urls: 
  #       - http://localhost:8000/v1
  #   max_tokens: 750
  #   temperature: 0.6
  #   model_seed: 0